我现在想要搭建一个本地LLM推理的服务，需求很简单：
1. 这是一个通过docker封装的容器，提供本地的推理api，docker使用的基础容器是`nvcr.io/nvidia/pytorch:24.10-py3`。
2. 推理接口请尽可能的与openai的接口保持一致，最好客户端只用改openai接口里的url，其余参数保持一致。客户端同样可能会是docker容器内发出的请求，需要考虑这一点。
3. 可以有多种LLM的模型推理接口，LLM的模型文件从我文件夹`/mnt/workspace/Source/LLM`获取，应该将这个文件夹在docker启动时映射到docker容器内的文件夹
4. LLM的权重是从hugging face上下载的，所有需要本地推理的LLM接口都需要是从hugging face上下载的，因此你应该可以用统一的部署框架，尽可能的实现我只需要从hugging face上下载新的模型文件，这个服务器就能支持这个模型的本地推理
5. 不同LLM的类型可以通过文件名判断，需要提供一个对外接口，说明现在支持了哪些本地推理的LLM类型，并且返回每个支持模型的推理调用的方式
6. 一台机器可能没有办法支持所有LLM的调用，这个时候可以考虑将上一个调用的LLM部署停掉
7. 我希望整个项目是简单的，你不用考虑其他太多情况，尽量保证部署、推理、API等都是标准的