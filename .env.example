# Local LLM Interface Configuration
# Copy this file to .env and adjust settings as needed

# Server Configuration
LLM_HOST=0.0.0.0
LLM_PORT=15530

# Model Configuration  
LLM_MODELS_DIR=/models
LLM_AUTO_UNLOAD_MODELS=true
LLM_MODEL_CACHE_SIZE=1

# GPU Configuration
LLM_GPU_MEMORY_UTILIZATION=0.9
LLM_CUDA_VISIBLE_DEVICES=0

# vLLM Configuration
LLM_VLLM_TENSOR_PARALLEL_SIZE=1
LLM_VLLM_DTYPE=auto
LLM_VLLM_MAX_MODEL_LEN=

# Transformers Configuration
LLM_TRANSFORMERS_DEVICE=cuda
LLM_TRANSFORMERS_TORCH_DTYPE=auto

# API Configuration
LLM_API_KEY=
LLM_CORS_ORIGINS=["*"]

# Logging
LLM_LOG_LEVEL=INFO