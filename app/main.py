"""FastAPI application entry point"""

import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger
import sys

from app.config import settings
from app.api import models, chat, embeddings, rerank
from models.manager import model_manager


# Configure logging
logger.remove()

# Console logging (colored)
logger.add(
    sys.stdout,
    level=settings.log_level,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)

# File logging (plain text)
logger.add(
    "/app/logs/app.log",
    level=settings.log_level,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    rotation="100 MB",
    retention="7 days",
    compression="zip"
)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    # Startup
    logger.info("Starting Local LLM Interface")
    logger.info(f"Models directory: {settings.models_dir}")
    logger.info(f"GPU memory utilization: {settings.gpu_memory_utilization}")
    
    # Refresh models on startup
    available_models = model_manager.get_available_models()
    logger.info(f"Found {len(available_models)} available models")
    
    yield
    
    # Shutdown
    logger.info("Shutting down Local LLM Interface")
    await model_manager.cleanup()


# Create FastAPI app
app = FastAPI(
    title="Local LLM Interface",
    description="""
    **OpenAI-compatible API for local LLM inference**
    
    æ”¯æŒChatã€Embeddingå’ŒRerankerä¸‰ç§æ¨¡åž‹ç±»åž‹çš„è‡ªåŠ¨åˆ‡æ¢å’Œæ™ºèƒ½èµ„æºç®¡ç†ã€‚
    ä¸“ä¸º8GBæ˜¾å­˜çŽ¯å¢ƒä¼˜åŒ–ï¼Œå®žçŽ°GPU+å†…å­˜æ··åˆéƒ¨ç½²ç­–ç•¥ã€‚
    
    ## ä¸»è¦ç‰¹æ€§
    
    - ðŸš€ **å®Œå…¨å…¼å®¹OpenAI API** - æ— ç¼æ›¿æ¢ï¼Œåªéœ€ä¿®æ”¹URL
    - ðŸ§  **æ™ºèƒ½æ¨¡åž‹åˆ‡æ¢** - è‡ªåŠ¨å¸è½½å’ŒåŠ è½½ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
    - âš¡ **æ··åˆéƒ¨ç½²ç­–ç•¥** - å¤§æ¨¡åž‹GPU+å†…å­˜ï¼Œå°æ¨¡åž‹çº¯GPU
    - ðŸ“Š **å®žæ—¶ç›‘æŽ§** - GPUå†…å­˜ä½¿ç”¨ã€æ¨¡åž‹çŠ¶æ€ä¸€ç›®äº†ç„¶
    
    ## æ”¯æŒçš„ç«¯ç‚¹
    
    - **Chat Completions** - `/v1/chat/completions`
    - **Embeddings** - `/v1/embeddings` 
    - **Reranking** - `/v1/rerank`
    - **Models** - `/v1/models`
    - **Health Check** - `/v1/health`
    """,
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(models.router)
app.include_router(chat.router)
app.include_router(embeddings.router)
app.include_router(rerank.router)


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Local LLM Interface - OpenAI Compatible API",
        "version": "0.1.0",
        "docs_url": "/docs",
        "health_url": "/v1/health",
        "models_url": "/v1/models"
    }


if __name__ == "__main__":
    import uvicorn
    
    logger.info(f"Starting server on {settings.host}:{settings.port}")
    uvicorn.run(
        "app.main:app",
        host=settings.host,
        port=settings.port,
        reload=False,
        log_level=settings.log_level.lower()
    )